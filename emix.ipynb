{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\varun\\OneDrive\\Desktop\\CourseMaterials\\NLP\\eda_nlp\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    BertForSequenceClassification,\n",
    "    BertTokenizerFast,\n",
    "    DistilBertForSequenceClassification,\n",
    "    DistilBertTokenizerFast,\n",
    "    DistilBertModel\n",
    ")\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import numpy as np\n",
    "import inspect\n",
    "device=torch.cuda.current_device()\n",
    "from EDA.augment import gen_eda\n",
    "from Utils.data_loader import get_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model_name='distilbert-base-uncased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = get_dataloader('Datasets/IMDB_500_sentiment.csv',batch_sizes=[16])[0]\n",
    "validation_dataloader=get_dataloader('Datasets/IMDB_1000_ssmba_val.csv')[0]\n",
    "ood_dataloader=get_dataloader('Datasets/SST-2_1000_ssmba_test.csv')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('Datasets/IMDB_Full.csv',names=['labels','text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizerFast.from_pretrained(pretrained_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import logging\n",
    "logging.set_verbosity_warning()\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class emixBERTClassifier(torch.nn.Module):\n",
    "    def __init__(self,model_name,num_labels,alpha=1,mixing=False,device=\"cuda\"):\n",
    "        super().__init__()\n",
    "        self.num_labels=num_labels\n",
    "        self.device=device\n",
    "        self.model_name=model_name\n",
    "        self.dbert=DistilBertForSequenceClassification.from_pretrained(self.model_name,num_labels=self.num_labels).to(self.device)\n",
    "        #self.tokenizer=DistilBertTokenizerFast.from_pretrained(self.model_name)\n",
    "        self.dropout=torch.nn.Dropout(0.1).to(self.device)\n",
    "        self.mixup_layers=torch.arange(1,self.dbert.distilbert.transformer.n_layers)\n",
    "        self.alpha=alpha\n",
    "        self.mixing=mixing\n",
    "\n",
    "    def get_mixing_ratio(self,std1,std2):\n",
    "        lam=torch.distributions.beta.Beta(self.alpha,self.alpha).sample()\n",
    "        t=1/(1+(std1/std2)*((1-lam)/lam))\n",
    "        return t.to(self.device)\n",
    "\n",
    "    def emix(self,h1,h2,a1,a2,t):\n",
    "        mixed_representation=(t*h1+(1-t)*h2)/torch.sqrt(t**2+(1-t)**2)\n",
    "        ## Original paper gives no information on handling attention masks\n",
    "        ## Assumption is made that the the \"and\" of attention mask is taken to avoid missing any data\n",
    "        mixed_attention=torch.max(a1,a2)\n",
    "        return mixed_representation.to(self.device),mixed_attention.to(self.device)\n",
    "        \n",
    "    def do_emix(self,option=None):\n",
    "        if option==None:\n",
    "            self.mixing= not self.mixing\n",
    "        else:\n",
    "            self.mixing=option\n",
    "\n",
    "    def forward(self,input_ids,attention_mask):\n",
    "        emb=self.dbert.distilbert.embeddings(input_ids)\n",
    "        \n",
    "        if self.mixing:\n",
    "            mixing_layer=np.random.choice(self.mixup_layers,size=1)[0]\n",
    "        else:\n",
    "            mixing_layer=0\n",
    "        hidden_rep=self.dbert.distilbert.transformer.layer[0](emb,attention_mask)[0]\n",
    "        t_list=[]\n",
    "        for layer_idx in range(1,self.dbert.distilbert.transformer.n_layers):\n",
    "            if layer_idx==mixing_layer:\n",
    "                mixed_states=[]\n",
    "                mixed_masks=[]\n",
    "                for i in range(input_ids.shape[0]//2):\n",
    "                    h_i=hidden_rep[i*2]\n",
    "                    h_j=hidden_rep[i*2+1]\n",
    "                    a_i=attention_mask[i*2]\n",
    "                    a_j=attention_mask[i*2+1]\n",
    "                    std_i=torch.std(h_i)\n",
    "                    std_j=torch.std(h_j)\n",
    "                    t=self.get_mixing_ratio(std_i,std_j)\n",
    "                    mixed_state,mixed_mask=self.emix(h_i,h_j,a_i,a_j,t)\n",
    "                    t_list.append(t)\n",
    "                    mixed_states.append(mixed_state)\n",
    "                    mixed_masks.append(mixed_mask)\n",
    "                hidden_rep=torch.stack(mixed_states).to(self.device)\n",
    "                attention_mask=torch.stack(mixed_masks).to(self.device)\n",
    "\n",
    "            # Performs dropout by default\n",
    "            hidden_rep=self.dbert.distilbert.transformer.layer[layer_idx](hidden_rep,attention_mask)[0]\n",
    "\n",
    "        pooled_output = hidden_rep[:, 0]\n",
    "        pooled_output=self.dbert.pre_classifier(pooled_output)\n",
    "        pooled_output = nn.ReLU()(pooled_output).to(self.device)  # (bs, dim)\n",
    "        pooled_output = self.dbert.dropout(pooled_output)  # (bs, dim)\n",
    "        logits=self.dbert.classifier(pooled_output)\n",
    "        return logits.to(self.device),t_list\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class emixRNNClassifier(torch.nn.Module):\n",
    "    def __init__(self,model_name,num_labels,vocab_size,model_type='GRU',hidden_size=128,embedding_dim=256,alpha=1,mixing=False,device=\"cuda\"):\n",
    "        super().__init__()\n",
    "        self.num_labels=num_labels\n",
    "        self.device=device\n",
    "        self.model_name=model_name\n",
    "        self.uses_attention=False\n",
    "        self.vocab_size=vocab_size\n",
    "        self.model_type=model_type\n",
    "        self.num_layers=3\n",
    "        self.embedding=nn.Embedding(vocab_size,embedding_dim).to(device)\n",
    "        self.rnns=nn.ModuleList()\n",
    "        if self.model_type=='GRU':\n",
    "            self.rnns=nn.ModuleList()\n",
    "            for i in range(self.num_layers):\n",
    "                input_size=embedding_dim if i==0 else hidden_size\n",
    "                self.rnns.append(nn.GRU(input_size,hidden_size,num_layers=1).to(device))\n",
    "        else:\n",
    "            for i in range(self.num_layers):\n",
    "                input_size=embedding_dim if i==0 else hidden_size\n",
    "                self.rnns.append(nn.LSTM(input_size,hidden_size,num_layers=1).to(device))\n",
    "        self.dropout=torch.nn.Dropout(0.1).to(self.device)\n",
    "        self.mixup_layers=torch.arange(1,self.num_layers)\n",
    "        self.alpha=alpha\n",
    "        self.mixing=mixing\n",
    "        self.fc=nn.Linear(hidden_size,num_labels).to(device)\n",
    "\n",
    "    def get_mixing_ratio(self,std1,std2):\n",
    "        lam=torch.distributions.beta.Beta(self.alpha,self.alpha).sample()\n",
    "        t=1/(1+(std1/std2)*((1-lam)/lam))\n",
    "        return t.to(self.device)\n",
    "\n",
    "    def emix(self,h1,h2,t):\n",
    "        mixed_representation=(t*h1+(1-t)*h2)/torch.sqrt(t**2+(1-t)**2)\n",
    "        \n",
    "        return mixed_representation.to(self.device)\n",
    "        \n",
    "    def do_emix(self,option=None):\n",
    "        if option==None:\n",
    "            self.mixing= not self.mixing\n",
    "        else:\n",
    "            self.mixing=option\n",
    "\n",
    "    def forward(self,input_ids,attention_mask):\n",
    "        emb=self.embedding(input_ids)\n",
    "        if self.mixing:\n",
    "            mixing_layer=np.random.choice(self.mixup_layers,size=1)[0]\n",
    "        else:\n",
    "            mixing_layer=0\n",
    "        \n",
    "        hidden_rep=self.rnns[0](emb)[0]\n",
    "        t_list=[]\n",
    "        for layer_idx in range(1,self.num_layers):\n",
    "            if layer_idx==mixing_layer:\n",
    "                mixed_states=[]\n",
    "                mixed_masks=[]\n",
    "                for i in range(input_ids.shape[0]//2):\n",
    "                    h_i=hidden_rep[i*2]\n",
    "                    h_j=hidden_rep[i*2+1]\n",
    "                    std_i=torch.std(h_i)\n",
    "                    std_j=torch.std(h_j)\n",
    "                    t=self.get_mixing_ratio(std_i,std_j)\n",
    "                    mixed_state=self.emix(h_i,h_j,t)\n",
    "                    t_list.append(t)\n",
    "                    mixed_states.append(mixed_state)\n",
    "                hidden_rep=torch.stack(mixed_states).to(self.device)\n",
    "            hidden_rep=self.rnns[layer_idx](hidden_rep)[0]\n",
    "            hidden_rep=self.dropout(hidden_rep)\n",
    "\n",
    "        final_state = hidden_rep[:,-1,:]\n",
    "        logits=self.fc(final_state)\n",
    "        return logits.to(self.device),t_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class emixTrainer():\n",
    "    def __init__(self,tokenizer,model,device,criterion,optimizer,train_dataloader,ood_dataloader,max_length=512,\n",
    "                 epochs=1,validation_dataloader=None,mixing=True):\n",
    "        self.model=model\n",
    "        self.tokenizer=tokenizer\n",
    "        self.device=device\n",
    "        self.criterion=criterion\n",
    "        self.optimizer=optimizer\n",
    "        self.ood_dataloader=ood_dataloader\n",
    "        self.train_dataloader=train_dataloader\n",
    "        self.validation_dataloader=validation_dataloader\n",
    "        self.epochs=epochs\n",
    "        self.mixing=mixing\n",
    "        self.max_length=max_length\n",
    "    \n",
    "    def mixup_criterion(self,preds,labels,t_list):\n",
    "        a_labs=[]\n",
    "        b_labs=[]\n",
    "        for i in range(len(preds)):\n",
    "            a_labs.append(labels[i*2])\n",
    "            b_labs.append(labels[2*i+1])\n",
    "        a_labs=torch.tensor(a_labs).to(self.device)\n",
    "        b_labs=torch.tensor(b_labs).to(self.device)\n",
    "        t=torch.tensor(t_list).to(self.device)\n",
    "        loss=(t*self.criterion(preds,a_labs)+(1-t)*self.criterion(preds,b_labs))/torch.sqrt(t**2+(1-t)**2)\n",
    "        loss=torch.mean(loss)\n",
    "        return loss\n",
    "               \n",
    "    def train(self,mix=True):\n",
    "        max_val_acc=0\n",
    "        best_epoch=-1\n",
    "        best_ood=0\n",
    "        for epoch in range(self.epochs):\n",
    "            epoch_loss=0\n",
    "            for text,labels in self.train_dataloader:\n",
    "                \n",
    "                inputs=tokenizer(text,padding='max_length', truncation=True, return_tensors='pt',\n",
    "                                        max_length=self.max_length)\n",
    "                \n",
    "                input_ids=inputs['input_ids'].to(self.device)\n",
    "                attention_mask=inputs['attention_mask'].to(self.device)\n",
    "                labels=labels.to(self.device)\n",
    "                ## Without mixing\n",
    "                self.optimizer.zero_grad()\n",
    "                self.model.do_emix(False)\n",
    "                preds,_=self.model(input_ids,attention_mask)\n",
    "                loss=self.criterion(preds,labels).to(self.device)\n",
    "                epoch_loss+=loss.item()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                if mix:\n",
    "                    ## With mixing\n",
    "                    self.optimizer.zero_grad()\n",
    "                    self.model.do_emix(True)\n",
    "                    preds,t_list=self.model(input_ids,attention_mask)\n",
    "                    loss=self.mixup_criterion(preds,labels,t_list).to(self.device)\n",
    "                    epoch_loss+=loss.item()\n",
    "                    loss.backward()\n",
    "                    self.optimizer.step()\n",
    "\n",
    "            if mix:    \n",
    "                avg_train_loss=epoch_loss/(len(self.train_dataloader)*1.5)\n",
    "            else:\n",
    "                avg_train_loss=epoch_loss/(len(self.train_dataloader))\n",
    "\n",
    "            if validation_dataloader is not None:\n",
    "                val_acc=self.evaluate(self.validation_dataloader)\n",
    "                if val_acc>max_val_acc:\n",
    "                    max_val_acc=val_acc\n",
    "                    best_epoch=epoch+1\n",
    "            else:\n",
    "                val_acc=0\n",
    "            train_acc=self.evaluate(self.train_dataloader)\n",
    "            \n",
    "            if (epoch+1)%5==0:\n",
    "                print(f'Epoch {epoch + 1}/{self.epochs}, Train Loss: {avg_train_loss}, Train Accuracy:{train_acc} Validation Accuracy: {val_acc}')\n",
    "        ood_acc=self.evaluate(self.ood_dataloader)\n",
    "        print(f\"Ood accuracy:{ood_acc}\")\n",
    "        return max_val_acc,ood_acc,best_epoch\n",
    "        \n",
    "    def evaluate(self,dataloader):\n",
    "        total_correct = 0\n",
    "        total_examples = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                texts, labels = batch\n",
    "                inputs = self.tokenizer(texts, padding='max_length', truncation=True, return_tensors='pt',\n",
    "                                        max_length=self.max_length)\n",
    "                input_ids = inputs[\"input_ids\"].to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "                attention_mask = inputs[\"attention_mask\"].to(self.device)\n",
    "                self.model.do_emix(False)\n",
    "                outputs,t_list = self.model(input_ids, attention_mask=attention_mask)\n",
    "                predictions = torch.argmax(outputs, dim=-1)\n",
    "                correct = (predictions == labels).sum().item()\n",
    "                total_correct += correct\n",
    "                total_examples += labels.size(0)\n",
    "            return total_correct / total_examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer=emixTrainer(tokenizer,emixBert,device=\"cuda\",criterion=criterion,optimizer=optimizer,train_dataloader=train_dataloader,validation_dataloader=validation_dataloader,epochs=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class emixRNNClassifier(torch.nn.Module):\n",
    "#    def __init__(self,model_name,num_labels,vocab_size,model_type='GRU',hidden_size=128,embedding_dim=256,alpha=1,mixing=False,device=\"cuda\"):"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_mixed_BERT_tests(train_dataloader,validation_dataloader,ood_dataloader,max_length,num_labels=2,epochs=10,mix=True,num_runs=5):\n",
    "    max_vals=[]\n",
    "    lr=3e-5\n",
    "    oods=[]\n",
    "    times=[]\n",
    "    model=\"BERT\"\n",
    "    condition=\"no_bias\"\n",
    "    for i in range(0,num_runs):\n",
    "        emixBert=emixBERTClassifier(pretrained_model_name,num_labels,device='cuda')\n",
    "        emixBert=emixBert.to('cuda')\n",
    "        optimizer=torch.optim.AdamW(emixBert.parameters(),lr=lr)\n",
    "        criterion=nn.CrossEntropyLoss()\n",
    "        trainer=emixTrainer(tokenizer,emixBert,device=\"cuda\",criterion=criterion,optimizer=optimizer,\n",
    "                            train_dataloader=train_dataloader,ood_dataloader=ood_dataloader,\n",
    "                            validation_dataloader=validation_dataloader,epochs=epochs,max_length=max_length)\n",
    "        \n",
    "        start_time=time.time()\n",
    "        max_val,ood_acc,best_epoch=trainer.train(mix=mix)\n",
    "        end_time=time.time()\n",
    "        print(\"-\"*50)\n",
    "        print(f\"{i+1} model: Max validation={max_val}, Best Epoch={best_epoch}, ood_acc={ood_acc}, time taken={end_time-start_time}\")\n",
    "        print(\"-\"*50)\n",
    "        times.append(end_time-start_time)\n",
    "        max_vals.append(max_val)\n",
    "        oods.append(ood_acc)\n",
    "        del emixBert\n",
    "        del optimizer\n",
    "        del criterion\n",
    "    torch.cuda.memory_allocated()\n",
    "    torch.cuda.empty_cache()\n",
    "    return np.mean(max_vals),np.std(max_vals),np.mean(times)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRU and LSTM Testing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_mixed_RNN_tests(model_type,train_dataloader,validation_dataloader,ood_dataloader,max_length,num_labels=2,mix=True,epochs=10,num_runs=5):\n",
    "    max_vals=[]\n",
    "    lr=3e-5\n",
    "    oods=[]\n",
    "    times=[]\n",
    "    for i in range(0,num_runs):\n",
    "        emixRNN=emixRNNClassifier(model_name=pretrained_model_name,num_labels=num_labels,\n",
    "                                  vocab_size=tokenizer.vocab_size,model_type=model_type,device='cuda')\n",
    "        emixRNN=emixRNN.to('cuda')\n",
    "        optimizer=torch.optim.AdamW(emixRNN.parameters(),lr=lr)\n",
    "        criterion=nn.CrossEntropyLoss()\n",
    "        trainer=emixTrainer(tokenizer,emixRNN,device=\"cuda\",criterion=criterion,optimizer=optimizer,\n",
    "                            train_dataloader=train_dataloader,ood_dataloader=ood_dataloader,\n",
    "                            validation_dataloader=validation_dataloader,epochs=epochs,max_length=max_length)\n",
    "        \n",
    "        start_time=time.time()\n",
    "        max_val,ood_acc,best_epoch=trainer.train(mix=mix)\n",
    "        end_time=time.time()\n",
    "        print(\"-\"*50)\n",
    "        print(f\"{i+1} model: Max validation={max_val}, Best Epoch={best_epoch}, ood_acc={ood_acc}, time taken={end_time-start_time}\")\n",
    "        print(\"-\"*50)\n",
    "        times.append(end_time-start_time)\n",
    "        max_vals.append(max_val)\n",
    "        oods.append(ood_acc)\n",
    "        del emixRNN\n",
    "        del optimizer\n",
    "        del criterion\n",
    "    torch.cuda.memory_allocated()\n",
    "    torch.cuda.empty_cache()\n",
    "    return np.mean(max_vals),np.std(max_vals),np.mean(times)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = get_dataloader('Datasets/IMDB_500.csv',batch_sizes=[16])[0]\n",
    "validation_dataloader=get_dataloader('Datasets/IMDB_1000_ssmba_val.csv')[0]\n",
    "ood_dataloader=get_dataloader('Datasets/SST-2_1000_ssmba_test.csv')[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_lengths=[32,256]\n",
    "mixes=[True]\n",
    "for length in max_lengths:\n",
    "    for mix in mixes:\n",
    "        avg_val,std_val,avg_time=run_mixed_BERT_tests(train_dataloader,validation_dataloader,ood_dataloader,\n",
    "                                              max_length=length,num_labels=3,epochs=10,mix=mix,num_runs=5)\n",
    "        with open(\"outputs.txt\",'a') as file:\n",
    "            if mix:\n",
    "                prefix=\"e\"\n",
    "            else:\n",
    "                prefix=\"no\"\n",
    "            file.write(f\"\\n{prefix}mix-BERT, Unbiased, IMDB, val_acc={avg_val}, stdc={std_val}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_lengths=[32,256]\n",
    "mixes=[True]\n",
    "model_types=['GRU','LSTM']\n",
    "for mtype in model_types:\n",
    "    for length in max_lengths:\n",
    "        for mix in mixes:\n",
    "            avg_val,std,avg_time=run_mixed_RNN_tests(mtype,train_dataloader,validation_dataloader,ood_dataloader,\n",
    "                                        max_length=length,num_labels=3,mix=mix,epochs=10,num_runs=5)\n",
    "            with open(\"MNLI_bias_outputs.txt\",'a') as file:\n",
    "                if mix:\n",
    "                    prefix=\"e\"\n",
    "                else:\n",
    "                    prefix=\"no\"\n",
    "                file.write(f\"\\n{prefix}mix-{mtype}, Unbiased, IMDB, val_acc={avg_val}, stdc={std}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = get_dataloader('Datasets/MNLI/MNLI_ssmba_train.csv',batch_sizes=[16])[0]\n",
    "validation_dataloader=get_dataloader('Datasets/MNLI/MNLI_ssmba_val.csv')[0]\n",
    "ood_dataloader=get_dataloader('Datasets/MNLI/MNLI_ssmba_test.csv')[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_lengths=[512,32]\n",
    "mixes=[True,False]\n",
    "\n",
    "for length in max_lengths:\n",
    "    for mix in mixes:\n",
    "        avg_val,avg_ood,avg_time=run_mixed_BERT_tests(train_dataloader,validation_dataloader,ood_dataloader,\n",
    "                                              max_length=length,num_labels=3,epochs=10,mix=mix,num_runs=5)\n",
    "        with open(\"MNLI_outputs.txt\",'a') as file:\n",
    "            if mix:\n",
    "                prefix=\"e\"\n",
    "            else:\n",
    "                prefix=\"no\"\n",
    "            file.write(f\"\\n{prefix}mix-BERT, Unbiased, window_size={length}, MNLI, val_acc={avg_val}, ood_acc={avg_ood}, time_taken={avg_time}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_lengths=[512,32]\n",
    "mixes=[True,False]\n",
    "model_types=['GRU','LSTM']\n",
    "for mtype in model_types:\n",
    "    for length in max_lengths:\n",
    "        for mix in mixes:\n",
    "            avg_val,avg_ood,avg_time=run_mixed_RNN_tests(mtype,train_dataloader,validation_dataloader,ood_dataloader,\n",
    "                                        max_length=length,num_labels=3,mix=mix,epochs=10,num_runs=5)\n",
    "            with open(\"MNLI_outputs.txt\",'a') as file:\n",
    "                if mix:\n",
    "                    prefix=\"e\"\n",
    "                else:\n",
    "                    prefix=\"no\"\n",
    "                file.write(f\"\\n{prefix}mix-{mtype}, Unbiased, window_size={length}, MNLI, val_acc={avg_val}, ood_acc={avg_ood}, time_taken={avg_time}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison with EDA and SSMBA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = get_dataloader('Datasets/IMDB_500.csv',batch_sizes=[16])[0]\n",
    "validation_dataloader=get_dataloader('Datasets/IMDB_1000_ssmba_val.csv')[0]\n",
    "ood_dataloader=get_dataloader('Datasets/SST-2_1000_ssmba_test.csv')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda_dataloader=get_dataloader('Datasets/IMDB_4500_eda8.csv')[0]\n",
    "ssmba_dataloader=get_dataloader('Datasets/IMDB_no_bias_8_ssmba_train.csv')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_runs=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods=[\n",
    "    'Normal',\n",
    "    'Emix',\n",
    "    'eda',\n",
    "    'ssmba'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for method in methods:\n",
    "    if method=='eda':\n",
    "        training_data=eda_dataloader\n",
    "    elif method=='ssmba':\n",
    "        training_data=ssmba_dataloader\n",
    "    else:\n",
    "        training_data=train_dataloader\n",
    "\n",
    "    if method=='Emix':\n",
    "        avg_val,avg_ood,avg_time=run_mixed_BERT_tests(training_data,validation_dataloader,ood_dataloader,\n",
    "                                                max_length=256,num_labels=2,epochs=10,mix=True,num_runs=num_runs)\n",
    "    else:\n",
    "        avg_val,avg_ood,avg_time=run_mixed_BERT_tests(training_data,validation_dataloader,ood_dataloader,\n",
    "                                                max_length=256,num_labels=2,epochs=10,mix=False,num_runs=num_runs)\n",
    "    with open(\"IMDB_comparison_outputs.txt\",'a') as file:\n",
    "        file.write(f\"\\n{method}-BERT, Unbiased, window_size=256, IMDB, val_acc={avg_val}, ood_acc={avg_ood}, time_taken={avg_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_types=['GRU','LSTM']\n",
    "for mtype in model_types:\n",
    "    for method in methods:\n",
    "        if method=='eda':\n",
    "            training_data=eda_dataloader\n",
    "        elif method=='ssmba':\n",
    "            training_data=ssmba_dataloader\n",
    "        else:\n",
    "            training_data=train_dataloader\n",
    "\n",
    "        if method=='Emix':\n",
    "            avg_val,avg_ood,avg_time=run_mixed_RNN_tests(mtype,train_dataloader,validation_dataloader,ood_dataloader,\n",
    "                                        max_length=256,num_labels=2,mix=True,epochs=10,num_runs=num_runs)\n",
    "        else:\n",
    "            avg_val,avg_ood,avg_time=run_mixed_RNN_tests(mtype,train_dataloader,validation_dataloader,ood_dataloader,\n",
    "                                        max_length=256,num_labels=2,mix=False,epochs=10,num_runs=num_runs)\n",
    "        with open(\"IMDB_comparison_outputs.txt\",'a') as file:\n",
    "            file.write(f\"\\n{method}-{mtype}, Unbiased, window_size=256, IMDB, val_acc={avg_val}, ood_acc={avg_ood}, time_taken={avg_time}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNLI SSMBA and EDA Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = get_dataloader('Datasets/MNLI/MNLI_ssmba_train.csv',batch_sizes=[16])[0]\n",
    "validation_dataloader=get_dataloader('Datasets/MNLI/MNLI_ssmba_val.csv')[0]\n",
    "ood_dataloader=get_dataloader('Datasets/MNLI/MNLI_ssmba_test.csv')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda_dataloader=get_dataloader('Datasets/MNLI_eda1.csv',batch_sizes=[16])[0]\n",
    "ssmba_dataloader=get_dataloader('Datasets/MNLI_no_bias_1_ssmba_train.csv')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods=[\n",
    "         'Normal',\n",
    "         'Emix',\n",
    "         'eda','ssmba']\n",
    "\n",
    "for method in methods:\n",
    "    if method=='eda':\n",
    "        training_data=eda_dataloader\n",
    "    elif method=='ssmba':\n",
    "        training_data=ssmba_dataloader\n",
    "    else:\n",
    "        training_data=train_dataloader\n",
    "\n",
    "    if method=='Emix':\n",
    "        avg_val,avg_ood,avg_time=run_mixed_BERT_tests(training_data,validation_dataloader,ood_dataloader,\n",
    "                                                max_length=256,num_labels=3,epochs=10,mix=True,num_runs=num_runs)\n",
    "    else:\n",
    "        avg_val,avg_ood,avg_time=run_mixed_BERT_tests(training_data,validation_dataloader,ood_dataloader,\n",
    "                                                max_length=256,num_labels=3,epochs=10,mix=False,num_runs=num_runs)\n",
    "    with open(\"MNLI_comparison_outputs.txt\",'a') as file:\n",
    "        file.write(f\"\\n{method}-BERT, Unbiased, window_size=256, MNLI, val_acc={avg_val}, ood_acc={avg_ood}, time_taken={avg_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda_dataloader=get_dataloader('Datasets/MNLI_eda8.csv',batch_sizes=[16])[0]\n",
    "ssmba_dataloader=get_dataloader('Datasets/MNLI_no_bias_8_ssmba_train.csv')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods=['eda','ssmba']\n",
    "model_types=['GRU','LSTM']\n",
    "for mtype in model_types:\n",
    "    for method in methods:\n",
    "        if method=='eda':\n",
    "            training_data=eda_dataloader\n",
    "        elif method=='ssmba':\n",
    "            training_data=ssmba_dataloader\n",
    "        else:\n",
    "            training_data=train_dataloader\n",
    "\n",
    "        if method=='Emix':\n",
    "            avg_val,avg_ood,avg_time=run_mixed_RNN_tests(mtype,train_dataloader,validation_dataloader,ood_dataloader,\n",
    "                                        max_length=256,num_labels=3,mix=True,epochs=10,num_runs=num_runs)\n",
    "        else:\n",
    "            avg_val,avg_ood,avg_time=run_mixed_RNN_tests(mtype,train_dataloader,validation_dataloader,ood_dataloader,\n",
    "                                        max_length=256,num_labels=3,mix=False,epochs=10,num_runs=num_runs)\n",
    "        with open(\"MNLI_comparison_outputs.txt\",'a') as file:\n",
    "            file.write(f\"\\n{method}-{mtype}, Unbiased, window_size=256, MNLI, val_acc={avg_val}, std={avg_ood}, time_taken={avg_time}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
